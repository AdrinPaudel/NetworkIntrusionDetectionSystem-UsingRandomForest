================================================================================
                  MODULE 3: DATA PREPROCESSING
                       DETAILED STEP-BY-STEP LOG
               Generated: 2026-01-24 18:48:09
================================================================================

This log provides a detailed chronological record of every preprocessing
operation performed, including before/after states, actions taken, and
validation checks.

================================================================================

STEP 1: DATA CLEANING
================================================================================

Purpose: Remove poor quality data (NaN, Inf, duplicates) and useless columns

[SUBSTEP 1.1] Initial State Assessment
  • Dataset shape: 16,233,002 rows × 80 columns
  • Memory usage: 11.01 GB
  • Action: Assess data quality issues

[SUBSTEP 1.2] Remove Useless Columns
  • Before: 80 columns
  • Columns to remove: Flow ID, Src IP, Dst IP, Src Port, Timestamp
  • Reason: Not useful for ML (identifiers, not features)
  • Action: df.drop(columns=[...], errors='ignore')
  • After: 75 columns

[SUBSTEP 1.3] Remove Bad 'Label' Class
  • Issue: Found 59 rows with label = 'Label' (misplaced header)
  • Action: df = df[df['Label'] != 'Label']
  • Rows removed: 59
  • Validation: No 'Label' values in Label column

[SUBSTEP 1.4] Identify NaN Values
  • Rows with NaN: 59,780 (0.370%)
  • Columns affected: 1
  • Decision: Remove all NaN rows (data loss acceptable)

[SUBSTEP 1.5] Remove NaN Values
  • Before: 16,233,002 rows
  • Action: df.dropna(inplace=True)
  • Removed: 59,780 rows
  • After: 16,173,222 rows
  • Validation: df.isna().sum().sum() == 0 ✓

[SUBSTEP 1.6] Identify Infinite Values
  • Rows with Inf: 36,039 (0.220%)
  • Columns affected: 2
  • Decision: Remove all Inf rows (data loss acceptable)

[SUBSTEP 1.7] Remove Infinite Values
  • Before: 16,173,222 rows
  • Action: df = df[~df.isin([np.inf, -np.inf]).any(axis=1)]
  • Removed: 36,039 rows
  • After: 16,137,183 rows
  • Validation: np.isinf(df.select_dtypes(include=[np.number])).sum().sum() == 0 ✓

[SUBSTEP 1.8] Identify Duplicate Rows
  • Duplicate rows: 4,157,778 (25.610%)
  • Decision: Remove all duplicates (keep first occurrence)

[SUBSTEP 1.9] Remove Duplicate Rows
  • Before: 16,137,183 rows
  • Action: df.drop_duplicates(inplace=True)
  • Removed: 4,157,778 rows
  • After: 11,979,405 rows
  • Validation: df.duplicated().sum() == 0 ✓

[SUBSTEP 1.10] Final Cleaning Summary
  • Initial rows: 16,233,002
  • Final rows: 11,979,405
  • Total removed: 4,253,597 (26.200%)
  • Memory saved: 3.10 GB
  • Quality: ✓ ACCEPTABLE
  • Checkpoint: Saved to cleaned_data.parquet

STEP 2: LABEL CONSOLIDATION
================================================================================

Purpose: Merge attack subcategories into parent classes (15 → 8)

[SUBSTEP 2.1] Analyze Original Labels
  • Unique labels found: 15
  • Action: df['Label'].value_counts()

[SUBSTEP 2.2] Define Consolidation Mapping
  • Mapping strategy: Group similar attacks
  • Mappings defined in config.LABEL_MAPPING:
      DDoS-LOIC-HTTP, DDoS-HOIC, etc. → DDoS
      DoS-Hulk, DoS-GoldenEye, etc. → DoS
      FTP-BruteForce, SSH-Bruteforce → Brute Force
      SQL Injection, XSS, Web → Web Attack
      Bot → Botnet
      Infilteration → Infiltration (typo fix)

[SUBSTEP 2.3] Apply Label Mapping
  • Action: df['Label'] = df['Label'].map(config.LABEL_MAPPING).fillna(df['Label'])
  • Before: 15 classes
  • After: 7 classes
  • Reduction: 8 classes (53.3%)

[SUBSTEP 2.4] Verify Consolidated Labels
  • Final distribution:
      Benign              : 10,628,038 (88.72%)
      DDoS                :    775,955 ( 6.48%)
      DoS                 :    196,568 ( 1.64%)
      Botnet              :    144,535 ( 1.21%)
      Infiltration        :    139,341 ( 1.16%)
      Brute Force         :     94,101 ( 0.79%)
      Web Attack          :        867 ( 0.01%)
  • Validation: All labels mapped correctly ✓

STEP 3: CATEGORICAL ENCODING
================================================================================

Purpose: Convert categorical features to numerical (required for ML)

[SUBSTEP 3.1] Identify Categorical Columns
  • Searching for: Protocol column
  • Searching for: Dst Port column
  • Found: True

[SUBSTEP 3.2] One-Hot Encode Protocol
  • Column: Protocol
  • Method: One-hot encoding (creates binary columns)
  • Action: pd.get_dummies(df, columns=['Protocol'], drop_first=False)
  • Before: 79 columns
  • Created columns:
      Protocol_0
      Protocol_17
      Protocol_6
  • After: 81 columns (+2)
  • Validation: Original Protocol column removed ✓

[SUBSTEP 3.3] Label Encode Target Variable
  • Column: Label
  • Method: Label encoding (string → integer)
  • Action: LabelEncoder().fit_transform(df['Label'])
  • Classes: 7
  • Mapping:
      0 ← Benign
      1 ← Botnet
      2 ← Brute Force
      3 ← DDoS
      4 ← DoS
      5 ← Infiltration
      6 ← Web Attack
  • Validation: All classes encoded ✓

[SUBSTEP 3.4] Verify No Categorical Columns Remain
  • Check: df.select_dtypes(include='object').columns
  • Result: No object columns remaining ✓
  • Checkpoint: Saved to train_encoded.parquet, test_encoded.parquet

STEP 4: TRAIN-TEST SPLIT
================================================================================

Purpose: Split dataset for training and testing (maintain class proportions)

[SUBSTEP 4.1] Separate Features and Labels
  • Features (X): All columns except Label
  • Labels (y): Label column
  • Feature count: 80
  • Total samples: 11,979,405

[SUBSTEP 4.2] Configure Split Parameters
  • Test size: 20% (0.2)
  • Stratify: Yes (maintain class proportions)
  • Random state: 42 (reproducibility)

[SUBSTEP 4.3] Perform Stratified Split
  • Action: train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
  • Training set: 9,583,524 samples (80.0%)
  • Test set: 2,395,881 samples (20.0%)

[SUBSTEP 4.4] Verify Stratification
  • Method: Compare class distributions in train vs test
  • Metric: Max absolute difference in class proportions
  • Result: 0.010% max difference
  • Threshold: <1% (excellent)
  • Status: ✓ VERIFIED
  • Conclusion: Train and test have same class proportions

STEP 5: FEATURE SCALING
================================================================================

Purpose: Normalize features using StandardScaler (mean=0, std=1)

[SUBSTEP 5.1] Select Scaler
  • Scaler: StandardScaler
  • Formula: (x - mean) / std
  • Result: mean ≈ 0, std ≈ 1
  • Configuration: config.SCALER_TYPE = 'standard'

[SUBSTEP 5.2] Fit Scaler on Training Data ONLY
  • Action: scaler.fit(X_train)
  • Training samples: 9,583,524
  • Features: 80
  • CRITICAL: Scaler learns statistics from TRAINING data ONLY
  • Purpose: Prevent data leakage (test data must not influence scaler)

[SUBSTEP 5.3] Transform Training Data
  • Action: X_train_scaled = scaler.transform(X_train)
  • Method: Apply learned mean/std from Step 5.2
  • Shape: (9583524, 80)
  • Validation: Mean ≈ 0, Std ≈ 1 ✓

[SUBSTEP 5.4] Transform Test Data
  • Action: X_test_scaled = scaler.transform(X_test)
  • Method: Apply TRAINING statistics (not test statistics)
  • Shape: (2395881, 80)
  • CRITICAL: Test data did NOT influence scaler
  • Result: No data leakage ✓

[SUBSTEP 5.5] Save Scaler Object
  • Action: joblib.dump(scaler, 'scaler.joblib')
  • Purpose: Reuse for new data in production
  • Checkpoint: Scaler saved successfully ✓

STEP 6: CLASS IMBALANCE HANDLING (SMOTE)
================================================================================

Purpose: Balance minority classes using synthetic oversampling

[SUBSTEP 6.1] Analyze Class Imbalance
  • Training samples: 9,583,524
  • Classes: 7
  • Distribution:
      Class 0:  8,502,430 (88.72%)
      Class 1:    115,628 ( 1.21%)
      Class 2:     75,281 ( 0.79%)
      Class 3:    620,764 ( 6.48%)
      Class 4:    157,254 ( 1.64%)
      Class 5:    111,473 ( 1.16%)
      Class 6:        694 ( 0.01%)

[SUBSTEP 6.2] Configure SMOTE Parameters
  • Strategy: Bring minorities to 3% of dataset (moderate)
  • k_neighbors: 5
  • Random state: 42
  • Classes to oversample: 6
  • CRITICAL: Apply to TRAINING data ONLY (test remains imbalanced)

[SUBSTEP 6.3] Apply SMOTE
  • Action: X_train_smoted, y_train_smoted = SMOTE(...).fit_resample(X_train, y_train)
  • Before: 9,583,524 samples
  • After: 10,398,554 samples
  • Synthetic samples: 815,030
  • Processing time: ~15-20 minutes (expected)

[SUBSTEP 6.4] Verify SMOTE Results
  • Final distribution:
      Class 0:  8,502,430 (81.77%)
      Class 1:    255,072 ( 2.45%) [+139,444, 2.2x]
      Class 2:    255,072 ( 2.45%) [+179,791, 3.4x]
      Class 3:    620,764 ( 5.97%)
      Class 4:    255,072 ( 2.45%) [+97,818, 1.6x]
      Class 5:    255,072 ( 2.45%) [+143,599, 2.3x]
      Class 6:    255,072 ( 2.45%) [+254,378, 367.5x]
  • Validation: Minority classes increased ✓
  • Checkpoint: Saved to train_scaled_smoted.parquet

================================================================================
                      PREPROCESSING COMPLETED SUCCESSFULLY
================================================================================

Total Steps: 6
Checkpoints Saved: 4
Models Saved: 1
Reports Generated: 2

Final Dataset Ready for Training:
  • Training: 10,398,554 samples
  • Test: 2,395,881 samples
  • Features: 80
  • Classes: 7

================================================================================
Next Module: Model Training (Module 4)
================================================================================
