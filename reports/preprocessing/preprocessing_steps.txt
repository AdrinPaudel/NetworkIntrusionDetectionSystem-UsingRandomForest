================================================================================
                  MODULE 3: DATA PREPROCESSING
                       DETAILED STEP-BY-STEP LOG
               Generated: 2026-01-26 11:06:03
================================================================================

This log provides a detailed chronological record of every preprocessing
operation performed, including before/after states, actions taken, and
validation checks.

================================================================================

STEP 1: DATA CLEANING
================================================================================

Purpose: Remove poor quality data (NaN, Inf, duplicates) and useless columns

[SUBSTEP 1.1] Initial State Assessment
  • Dataset shape: 16,232,943 rows × 80 columns
  • Memory usage: 0.00 GB
  • Action: Assess data quality issues

[SUBSTEP 1.2] Remove Useless Columns
  • Before: 80 columns
  • Columns dropped: 0
  • Reason: Not useful for ML (identifiers, not features)
  • Action: df.drop(columns=[...], errors='ignore')
  • After: 80 columns

[SUBSTEP 1.3] Remove Bad 'Label' Class
  • Issue: Rows with label = 'Label' (misplaced header)
  • Action: df = df[df['Label'] != 'Label']
  • Rows removed: 0
  • Validation: No 'Label' values in Label column

[SUBSTEP 1.4] Identify NaN Values
  • Rows with NaN: 0 (0.368%)
  • Columns affected: 0
  • Decision: Remove all NaN rows (data loss acceptable)

[SUBSTEP 1.5] Remove NaN Values
  • Before: 16,232,943 rows
  • Action: df.dropna(inplace=True)
  • Removed: 0 rows
  • After: 16,232,943 rows
  • Validation: df.isna().sum().sum() == 0 ✓

[SUBSTEP 1.6] Identify Infinite Values
  • Rows with Inf: 0 (0.222%)
  • Columns affected: 0
  • Decision: Remove all Inf rows (data loss acceptable)

[SUBSTEP 1.7] Remove Infinite Values
  • Before: 16,232,943 rows
  • Action: df = df[~df.isin([np.inf, -np.inf]).any(axis=1)]
  • Removed: 0 rows
  • After: 16,232,943 rows
  • Validation: np.isinf(df.select_dtypes(include=[np.number])).sum().sum() == 0 ✓

[SUBSTEP 1.8] Identify Duplicate Rows
  • Duplicate rows: 0 (0.000%)
  • Decision: Remove all duplicates (keep first occurrence)

[SUBSTEP 1.9] Remove Duplicate Rows
  • Before: 16,232,943 rows
  • Action: df.drop_duplicates(inplace=True)
  • Removed: 0 rows
  • After: 11,979,405 rows
  • Validation: df.duplicated().sum() == 0 ✓

[SUBSTEP 1.10] Final Cleaning Summary
  • Initial rows: 16,232,943
  • Final rows: 11,979,405
  • Total removed: 4,253,538 (26.203%)
  • Memory saved: 0.00 GB
  • Quality: ✓ ACCEPTABLE
  • Checkpoint: Saved to cleaned_data.parquet

STEP 2: LABEL CONSOLIDATION
================================================================================

Purpose: Merge attack subcategories into parent classes (15 → 8)

[SUBSTEP 2.1] Analyze Original Labels
  • Unique labels found: 15
  • Action: df['Label'].value_counts()
  • Distribution:
      Benign                   : 10,628,038 (88.72%)
      DDoS attacks-LOIC-HTTP   :    575,364 ( 4.80%)
      DDOS attack-HOIC         :    198,861 ( 1.66%)
      DoS attacks-Hulk         :    145,199 ( 1.21%)
      Bot                      :    144,535 ( 1.21%)
      Infilteration            :    139,341 ( 1.16%)
      SSH-Bruteforce           :     94,048 ( 0.79%)
      DoS attacks-GoldenEye    :     41,406 ( 0.35%)
      DoS attacks-Slowloris    :      9,908 ( 0.08%)
      DDOS attack-LOIC-UDP     :      1,730 ( 0.01%)
      Brute Force -Web         :        555 ( 0.00%)
      Brute Force -XSS         :        228 ( 0.00%)
      SQL Injection            :         84 ( 0.00%)
      DoS attacks-SlowHTTPTest :         55 ( 0.00%)
      FTP-BruteForce           :         53 ( 0.00%)

[SUBSTEP 2.2] Define Consolidation Mapping
  • Mapping strategy: Group similar attacks
  • Mappings defined in config.LABEL_MAPPING:
      DDoS-LOIC-HTTP, DDoS-HOIC, etc. → DDoS
      DoS-Hulk, DoS-GoldenEye, etc. → DoS
      FTP-BruteForce, SSH-Bruteforce → Brute Force
      SQL Injection, XSS, Web → Web Attack
      Bot → Botnet
      Infilteration → Infiltration (typo fix)

[SUBSTEP 2.3] Apply Label Mapping
  • Action: df['Label'] = df['Label'].map(config.LABEL_MAPPING).fillna(df['Label'])
  • Before: 15 classes
  • Mapped to: 6 classes (includes __DROP__)

[SUBSTEP 2.4] Remove Rows Marked for Dropping
  • Rows marked __DROP__ (e.g., SQL Injection): 84
  • Action: df = df[df['Label'] != '__DROP__']
  • Removed: 84 rows
  • After: 5 classes (final)

[SUBSTEP 2.5] Verify Consolidated Labels
  • Final distribution:
      Benign              : 10,628,038 (88.72%)
      DDoS                :    775,955 ( 6.48%)
      DoS                 :    196,568 ( 1.64%)
      Botnet              :    144,535 ( 1.21%)
      Infilteration       :    139,341 ( 1.16%)
      Brute Force         :     94,884 ( 0.79%)
  • Validation: All labels mapped correctly ✓

STEP 3: CATEGORICAL ENCODING
================================================================================

Purpose: Convert categorical features to numerical (required for ML)

[SUBSTEP 3.1] Identify Categorical Columns
  • Searching for: Protocol column
  • Searching for: Dst Port column
  • Found: True

[SUBSTEP 3.2] One-Hot Encode Protocol
  • Column: Protocol
  • Method: One-hot encoding (creates binary columns)
  • Action: pd.get_dummies(df, columns=['Protocol'], drop_first=False)
  • Before: 79 columns
  • Created columns:
      Protocol_0
      Protocol_17
      Protocol_6
  • After: 81 columns (+2)
  • Validation: Original Protocol column removed ✓

[SUBSTEP 3.3] Label Encode Target Variable
  • Column: Label
  • Method: Label encoding (string → integer)
  • Action: LabelEncoder().fit_transform(df['Label'])
  • Classes: 6
  • Mapping:
      0 ← Benign
      1 ← Botnet
      2 ← Brute Force
      3 ← DDoS
      4 ← DoS
      5 ← Infilteration
  • Validation: All classes encoded ✓

[SUBSTEP 3.4] Verify No Categorical Columns Remain
  • Check: df.select_dtypes(include='object').columns
  • Result: No object columns remaining ✓
  • Checkpoint: Saved to train_encoded.parquet, test_encoded.parquet

STEP 4: TRAIN-TEST SPLIT
================================================================================

Purpose: Split dataset for training and testing (maintain class proportions)

[SUBSTEP 4.1] Separate Features and Labels
  • Features (X): All columns except Label
  • Labels (y): Label column
  • Feature count: 80
  • Total samples: 11,979,321

[SUBSTEP 4.2] Configure Split Parameters
  • Test size: 20% (0.2)
  • Stratify: Yes (maintain class proportions)
  • Random state: 42 (reproducibility)

[SUBSTEP 4.3] Perform Stratified Split
  • Action: train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
  • Training set: 9,583,456 samples (80.0%)
  • Test set: 2,395,865 samples (20.0%)

[SUBSTEP 4.4] Verify Stratification
  • Method: Compare class distributions in train vs test
  • Metric: Max absolute difference in class proportions
  • Result: 0.000% max difference
  • Threshold: <1% (excellent)
  • Status: ✓ VERIFIED
  • Conclusion: Train and test have same class proportions

STEP 5: FEATURE SCALING
================================================================================

Purpose: Normalize features using StandardScaler (mean=0, std=1)

[SUBSTEP 5.1] Select Scaler
  • Scaler: StandardScaler
  • Formula: (x - mean) / std
  • Result: mean ≈ 0, std ≈ 1
  • Configuration: config.SCALER_TYPE = 'standard'

[SUBSTEP 5.2] Fit Scaler on Training Data ONLY
  • Action: scaler.fit(X_train)
  • Training samples: 9,583,456
  • Features: 80
  • CRITICAL: Scaler learns statistics from TRAINING data ONLY
  • Purpose: Prevent data leakage (test data must not influence scaler)

[SUBSTEP 5.3] Transform Training Data
  • Action: X_train_scaled = scaler.transform(X_train)
  • Method: Apply learned mean/std from Step 5.2
  • Shape: (9583456, 80)
  • Validation: Mean ≈ 0, Std ≈ 1 ✓

[SUBSTEP 5.4] Transform Test Data
  • Action: X_test_scaled = scaler.transform(X_test)
  • Method: Apply TRAINING statistics (not test statistics)
  • Shape: (2395865, 80)
  • CRITICAL: Test data did NOT influence scaler
  • Result: No data leakage ✓

[SUBSTEP 5.5] Save Scaler Object
  • Action: joblib.dump(scaler, 'scaler.joblib')
  • Purpose: Reuse for new data in production
  • Checkpoint: Scaler saved successfully ✓

STEP 6: CLASS IMBALANCE HANDLING (SMOTE)
================================================================================

Purpose: Balance minority classes using synthetic oversampling

[SUBSTEP 6.1] Analyze Class Imbalance
  • Training samples: 9,583,456
  • Classes: 6
  • Distribution:
      Class 0:  8,502,430 (88.72%)
      Class 1:    115,628 ( 1.21%)
      Class 2:     75,907 ( 0.79%)
      Class 3:    620,764 ( 6.48%)
      Class 4:    157,254 ( 1.64%)
      Class 5:    111,473 ( 1.16%)

[SUBSTEP 6.2] Configure SMOTE Parameters
  • Strategy: Bring minorities to 3% of dataset (moderate)
  • k_neighbors: 5
  • Random state: 42
  • Classes to oversample: 3
  • CRITICAL: Apply to TRAINING data ONLY (test remains imbalanced)

[SUBSTEP 6.3] Apply SMOTE
  • Action: X_train_smoted, y_train_smoted = SMOTE(...).fit_resample(X_train, y_train)
  • Before: 9,583,456 samples
  • After: 9,759,619 samples
  • Synthetic samples: 176,163
  • Processing time: ~15-20 minutes (expected)

[SUBSTEP 6.4] Verify SMOTE Results
  • Final distribution:
      Class 0:  8,502,430 (87.12%)
      Class 1:    143,751 ( 1.47%) [+28,123, 1.2x]
      Class 2:    191,669 ( 1.96%) [+115,762, 2.5x]
      Class 3:    620,764 ( 6.36%)
      Class 4:    157,254 ( 1.61%)
      Class 5:    143,751 ( 1.47%) [+32,278, 1.3x]
  • Validation: Minority classes increased ✓
  • Checkpoint: Saved to train_scaled_smoted.parquet

================================================================================
                      PREPROCESSING COMPLETED SUCCESSFULLY
================================================================================

Total Steps: 6
Checkpoints Saved: 4
Models Saved: 1
Reports Generated: 2

Final Dataset Ready for Training:
  • Training: 9,759,619 samples
  • Test: 2,395,865 samples
  • Features: 80
  • Classes: 6

================================================================================
Next Module: Model Training (Module 4)
================================================================================
